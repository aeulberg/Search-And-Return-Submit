[//]: # (Image References)
[image0]: ./misc/rover_image.jpg
[image1]: ./misc/Threshed1.png
[image2]: ./misc/warpandmask.png
[image3]: ./misc/rock1.png
[image4]: ./misc/rock2.png
[image5]: ./misc/coordinateTransform.png

## Project: Search and Sample Return
### Andrew Eulberg
---
![alt text][image0]

**The goals / steps of this project are the following:**  

**Training / Calibration**  

* Download the simulator and take data in "Training Mode"
* Test out the functions in the Jupyter Notebook provided
* Add functions to detect obstacles and samples of interest (golden rocks)
* Fill in the `process_image()` function with the appropriate image processing steps (perspective transform, color threshold etc.) to get from raw images to a map.  The `output_image` you create in this step should demonstrate that your mapping pipeline works.
* Use `moviepy` to process the images in your saved dataset with the `process_image()` function.  Include the video you produce as part of your submission.

**Autonomous Navigation / Mapping**

* Fill in the `perception_step()` function within the `perception.py` script with the appropriate image processing functions to create a map and update `Rover()` data (similar to what you did with `process_image()` in the notebook). 
* Fill in the `decision_step()` function within the `decision.py` script with conditional statements that take into consideration the outputs of the `perception_step()` in deciding how to issue throttle, brake and steering commands. 
* Iterate on your perception and decision function until your rover does a reasonable (need to define metric) job of navigating and mapping.  




## [Rubric](https://review.udacity.com/#!/rubrics/916/view) Points
### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  

You're reading it!

### Notebook Analysis
#### 1. Prespective Transform
The prespective transform was given in class along with the source and destination coordinates.  I made modifications to it after seeing a mask returned in the project walkthrough video so mine now produces a mask with the warped image.  The masks purposes is to prevent the pixels lost from the transformation counting as objects (obstacles,rocks).  The lost pixels can be observed in black on the left image below and also black on the mask to the right. 
![alt text][image2]

The mask is also edited to restrict the angle of the cone seen by the camera on the idea that boundaries of the camera image are more distorted after transforming and will distort the data as well. Its talked more extensively in the section about `perception_step()`.  The mask is generated by transforming an image full of ones where the pixels should be.  By reducing the vertical edges, the cone shape on the mask will constrict; the horizontal edges will reduce the horizontal edges of the mask but not in a linear fashion.  Its just easier to adjust the horizontal edges after the transformation.

Since I was editing the boundries of the mask, my mask image is created first full of zeros based on the original image dimensions and then populated with ones based on the targeted effect.  The example from the walkthrough did not need to adjust anything so their mask image was just created full of ones.
#### 2. Threshholding image
The threshholding method was also discussed in class, where an identically size image is created by comparing every pixel and highlighting every pixel that had a color in all 3 channels greater then 160 (out of 255).  After some manipulation I felt the green channel needed to be increased to 170.  The image below is the threshheld version to the left image from the previous section.

![alt text][image1]

#### 3. Finding Rocks
Finding rocks was important for picking out the special rocks on the camera image and was handled by the function `find rocks()`.  The function was based off the threshhold function (`color_thresh()`) with minor modifications the would look for a specified range of colors in each color channel of an image.  The specifics wond up being red greater then 100, green greater then 105 and blue less then 90.  These values were found by observing the images below until on other objects appeared on the right image below and the the rock its self was mostly solid.  The images below show a rock on normal terrain.

![alt text][image3]

The image below was the second test case to ensure that the ranges did not include any terrain as rocks.
![alt text][image4]

#### 4. Coordinate Transformations
The coordinate transforms were discussed inclass and included in the notebook for testing.  The image below shows the camera image(upper left), the transformed version of it(upper right),the threshheld verion(lower left) and the calculated coordinate graph(lower right).  The threshed image is converted to x and y coordinates and then an arrow is super imposed to show the mean direction of the same points in polar form.
![alt text][image5]

#### 5. `process_image()` function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap.  Run `process_image()` on your test data using the `moviepy` functions provided to create video output of your result. 
And another! 


### Autonomous Navigation and Mapping

#### 1. Fill in the `perception_step()` (at the bottom of the `perception.py` script) and `decision_step()` (in `decision.py`) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.


#### 2. Launching in autonomous mode your rover can navigate and map autonomously.  Explain your results and how you might improve them in your writeup.  

**Note: running the simulator with different choices of resolution and graphics quality may produce different results, particularly on different machines!  Make a note of your simulator settings (resolution and graphics quality set on launch) and frames per second (FPS output to terminal by `drive_rover.py`) in your writeup when you submit the project so your reviewer can reproduce your results.**

Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  




